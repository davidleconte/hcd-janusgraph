# AlertManager Configuration
# File: alertmanager.yml
# Created: 2026-01-29
# Purpose: Configure alert routing and notification channels

global:
  resolve_timeout: 5m
  smtp_from: 'alertmanager@janusgraph.local'
  smtp_smarthost: 'smtp.example.com:587'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true
  
  # Slack webhook (configure via environment variable)
  slack_api_url: '${SLACK_WEBHOOK_URL}'

# Route tree for alert distribution
route:
  # Default receiver for all alerts
  receiver: 'team-notifications'
  
  # Group alerts by these labels
  group_by: ['alertname', 'cluster', 'service', 'severity']
  
  # Wait time before sending first notification for a group
  group_wait: 10s
  
  # Wait time before sending notification about new alerts in a group
  group_interval: 10s
  
  # Wait time before re-sending a notification
  repeat_interval: 12h
  
  # Child routes for specific alert types
  routes:
    # Critical alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 4h
      
    # Security alerts - send to security team
    - match:
        category: security
      receiver: 'security-team'
      group_wait: 5s
      repeat_interval: 6h
      
    # Compliance alerts - send to compliance team
    - match:
        category: compliance
      receiver: 'compliance-team'
      repeat_interval: 24h
      
    # Performance warnings - less urgent
    - match:
        severity: warning
        category: performance
      receiver: 'performance-team'
      repeat_interval: 24h

# Notification receivers
receivers:
  # Default team notifications (email + Slack)
  - name: 'team-notifications'
    email_configs:
      - to: 'ops-team@example.com'
        headers:
          Subject: '[JanusGraph] {{ .GroupLabels.alertname }}'
        html: |
          <h2>Alert: {{ .GroupLabels.alertname }}</h2>
          <p><strong>Severity:</strong> {{ .GroupLabels.severity }}</p>
          <p><strong>Cluster:</strong> {{ .GroupLabels.cluster }}</p>
          {{ range .Alerts }}
          <hr>
          <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Started:</strong> {{ .StartsAt }}</p>
          {{ end }}
    slack_configs:
      - channel: '#janusgraph-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          *Severity:* {{ .GroupLabels.severity }}
          *Cluster:* {{ .GroupLabels.cluster }}
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        color: '{{ if eq .GroupLabels.severity "critical" }}danger{{ else if eq .GroupLabels.severity "warning" }}warning{{ else }}good{{ end }}'
        send_resolved: true
  
  # Critical alerts - multiple channels
  - name: 'critical-alerts'
    email_configs:
      - to: 'ops-team@example.com,oncall@example.com'
        headers:
          Subject: '[CRITICAL] {{ .GroupLabels.alertname }}'
          Priority: 'urgent'
        html: |
          <h1 style="color: red;">CRITICAL ALERT</h1>
          <h2>{{ .GroupLabels.alertname }}</h2>
          {{ range .Alerts }}
          <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Service:</strong> {{ .Labels.service }}</p>
          <p><strong>Started:</strong> {{ .StartsAt }}</p>
          {{ end }}
    slack_configs:
      - channel: '#janusgraph-critical'
        title: ':rotating_light: CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          @channel Critical alert requires immediate attention!
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Service:* {{ .Labels.service }}
          {{ end }}
        color: 'danger'
        send_resolved: true
  
  # Security team notifications
  - name: 'security-team'
    email_configs:
      - to: 'security-team@example.com'
        headers:
          Subject: '[SECURITY] {{ .GroupLabels.alertname }}'
        html: |
          <h2 style="color: orange;">Security Alert</h2>
          <h3>{{ .GroupLabels.alertname }}</h3>
          {{ range .Alerts }}
          <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
          {{ end }}
    slack_configs:
      - channel: '#security-alerts'
        title: ':shield: Security Alert: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          {{ end }}
        color: 'warning'
  
  # Compliance team notifications
  - name: 'compliance-team'
    email_configs:
      - to: 'compliance-team@example.com'
        headers:
          Subject: '[COMPLIANCE] {{ .GroupLabels.alertname }}'
        html: |
          <h2>Compliance Alert</h2>
          <h3>{{ .GroupLabels.alertname }}</h3>
          {{ range .Alerts }}
          <p><strong>Summary:</strong> {{ .Annotations.summary }}</p>
          <p><strong>Description:</strong> {{ .Annotations.description }}</p>
          {{ end }}
  
  # Performance team notifications
  - name: 'performance-team'
    email_configs:
      - to: 'performance-team@example.com'
        headers:
          Subject: '[PERFORMANCE] {{ .GroupLabels.alertname }}'
    slack_configs:
      - channel: '#performance-alerts'
        title: 'Performance Alert: {{ .GroupLabels.alertname }}'
        color: 'warning'

# Inhibition rules - suppress certain alerts when others are firing
inhibit_rules:
  # Suppress warning alerts when critical alerts are firing for the same service
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
  
  # Suppress individual service alerts when the entire cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match_re:
      alertname: '.*Down'
    equal: ['cluster']
  
  # Suppress performance alerts when service is down
  - source_match:
      alertname: 'ServiceDown'
    target_match:
      category: 'performance'
    equal: ['service', 'instance']

# Made with Bob