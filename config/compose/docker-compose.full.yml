# File: docker-compose.full.yml
# NOTE: This is the DEVELOPMENT configuration with permissive defaults.
# For PRODUCTION, use: docker-compose -f docker-compose.full.yml -f docker-compose.prod.yml up -d
# Created: 2026-01-28T10:37:00.567
# Author: David LECONTE - IBM Worldwide | Data & AI | Tiger Team | Data Watstonx.Data Global Product Specialist (GPS)
#

# Complete HCD + JanusGraph Stack with Visualization Tools
# Platform: macOS M3 Pro (ARM64)
# Podman machine: podman-wxd

version: '3.8'

networks:
  hcd-janusgraph-network:
    driver: bridge

volumes:
  hcd-data:
  hcd-commitlog:
  hcd-logs:
  opensearch-data:
  janusgraph-db:
  jupyter-notebooks:
  prometheus-data:
  grafana-data:
  vault-data:
  vault-logs:
  alertmanager-data:
  pulsar-data:

services:
  # ============================================================================
  # CORE STACK
  # ============================================================================

  hcd-server:
    image: localhost/hcd:1.2.3
    hostname: hcd-server
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    networks:
      hcd-janusgraph-network:
        aliases:
          - hcd-server
    ports:
      - "19042:9042"    # CQL
      - "17000-17001:7000-7001"  # Inter-node
      - "17199:7199"    # JMX (for Prometheus)
      - "19160:9160"    # Thrift (legacy)
    volumes:
      - hcd-data:/var/lib/hcd/data
      - hcd-commitlog:/var/lib/hcd/commitlog
      - hcd-logs:/var/log/cassandra
    environment:
      - CASSANDRA_CLUSTER_NAME=HCD-Cluster
      - CASSANDRA_DC=datacenter1
      - CASSANDRA_RACK=rack1
      - MAX_HEAP_SIZE=4G
      - HEAP_NEWSIZE=800M
    healthcheck:
      test: ["CMD", "/opt/hcd/bin/nodetool", "status"]
      interval: 30s
      timeout: 30s
      retries: 10
      start_period: 180s
    restart: unless-stopped

  opensearch:
    build:
      context: ../..
      dockerfile: docker/opensearch/Dockerfile
    image: localhost/opensearch-jvector:latest
    hostname: opensearch
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    networks:
      - hcd-janusgraph-network
    ports:
      - "9200:9200"      # REST API
      - "9600:9600"      # Performance Analyzer
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - plugins.security.disabled=true  # Dev mode - enable in production
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=${OPENSEARCH_INITIAL_ADMIN_PASSWORD:?OPENSEARCH_INITIAL_ADMIN_PASSWORD must be set in .env file}

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - opensearch-data:/usr/share/opensearch/data
      - ../../config/opensearch/jvector-install.sh:/tmp/jvector-install.sh:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:9200/_cluster/health | grep -qE '\"status\":\"(green|yellow)\"'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  opensearch-dashboards:
    image: docker.io/opensearchproject/opensearch-dashboards:3.4.0
    hostname: opensearch-dashboards
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    ports:
      - "5601:5601"    # OpenSearch Dashboards UI
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
    depends_on:
      opensearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'available'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  janusgraph-server:
    image: docker.io/janusgraph/janusgraph:1.0.0
    hostname: janusgraph-server
    environment:
      - JAVA_OPTIONS=-Xms1g -Xmx3g -XX:+UseG1GC
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 6G
        reservations:
          cpus: '1.0'
          memory: 3G
    networks:
      hcd-janusgraph-network:
        aliases:
          - janusgraph-server
    ports:
      - "18182:8182"    # Gremlin WebSocket
      - "18184:8184"    # Management (optional)
    volumes:
      - ../../config/janusgraph/janusgraph-hcd.properties:/opt/janusgraph/conf/janusgraph-hcd.properties:ro
      - ../../config/janusgraph/janusgraph-server-hcd.yaml:/opt/janusgraph/conf/janusgraph-server-hcd.yaml:ro
      - ../../config/janusgraph/janusgraph-init.groovy:/opt/janusgraph/scripts/janusgraph-init.groovy:ro
      - janusgraph-db:/var/lib/janusgraph/db
    command: >
      bash -c "
      echo 'Waiting for HCD to be fully ready...' &&
      sleep 20 &&
      echo 'Starting JanusGraph with HCD backend...' &&
      export JAVA_OPTS='$${JAVA_OPTIONS}' &&
      /opt/janusgraph/bin/janusgraph-server.sh /opt/janusgraph/conf/janusgraph-server-hcd.yaml
      "
    depends_on:
      hcd-server:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'cat < /dev/null > /dev/tcp/127.0.0.1/8182' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    restart: unless-stopped

  gremlin-console:
    image: docker.io/janusgraph/janusgraph:1.0.0
    hostname: gremlin-console
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - hcd-janusgraph-network
    volumes:
      - ../../config/janusgraph/janusgraph-hcd.properties:/opt/janusgraph/conf/janusgraph-hcd.properties:ro
      - ../../config/janusgraph/remote-connect.properties:/opt/janusgraph/conf/remote-connect.properties:ro
    environment:
      - GREMLIN_REMOTE_HOSTS=janusgraph-server
    entrypoint: ["tail", "-f", "/dev/null"]
    depends_on:
      janusgraph-server:
        condition: service_healthy
    stdin_open: true
    tty: true
    restart: "no"

  # ============================================================================
  # STREAMING (Event-Sourced Architecture)
  # ============================================================================

  pulsar:
    image: docker.io/apachepulsar/pulsar:3.2.0
    hostname: pulsar
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    networks:
      - hcd-janusgraph-network
    ports:
      - "6650:6650"     # Pulsar protocol (binary)
      - "8081:8080"     # HTTP admin API (using 8081 to avoid conflict with graphexp)
    environment:
      - PULSAR_MEM=-Xms512m -Xmx1g
      - PULSAR_STANDALONE_USE_ZOOKEEPER=1
      # Fix for Apple Silicon (M1/M2/M3/M4) - JDK-8345296
      - _JAVA_OPTIONS=-XX:UseSVE=0
    command: >
      bash -c "
      echo 'Starting Pulsar with advertised-address localhost (fix for macOS networking)...' &&
      bin/pulsar standalone --advertised-address localhost &
      sleep 45 &&
      echo 'Creating banking namespace and topics...' &&
      bin/pulsar-admin namespaces create public/banking 2>/dev/null || true &&
      bin/pulsar-admin namespaces set-deduplication public/banking --enable 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/persons-events 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/accounts-events 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/transactions-events 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/companies-events 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/communications-events 2>/dev/null || true &&
      bin/pulsar-admin topics create persistent://public/banking/dlq-events 2>/dev/null || true &&
      echo 'Pulsar topics created successfully!' &&
      wait
      "
    volumes:
      - pulsar-data:/pulsar/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:8080/admin/v2/clusters | grep -q standalone"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    restart: unless-stopped

  pulsar-cli:
    image: docker.io/apachepulsar/pulsar:3.2.0
    hostname: pulsar-cli
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - hcd-janusgraph-network
    environment:
      # Fix for Apple Silicon (M1/M2/M3/M4) - JDK-8345296
      - _JAVA_OPTIONS=-XX:UseSVE=0
    entrypoint: ["sleep", "infinity"]
    depends_on:
      pulsar:
        condition: service_healthy
    restart: unless-stopped

  # ============================================================================
  # VISUALIZATION & ANALYSIS
  # ============================================================================

  jupyter:
    build:
      context: ../..
      dockerfile: docker/jupyter/Dockerfile
    image: localhost/jupyter-janusgraph:latest
    hostname: jupyter-lab
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    networks:
      - hcd-janusgraph-network
    ports:
      - "8888:8888"    # Jupyter Lab
    volumes:
      - jupyter-notebooks:/workspace/notebooks
      - ../../banking:/workspace/banking
      - ../../src/python:/workspace/src/python
      - ../../exports:/workspace/exports
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - GREMLIN_URL=ws://janusgraph-server:8182/gremlin
      - HCD_HOST=hcd-server
      - HCD_PORT=9042
    depends_on:
      janusgraph-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/api"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  janusgraph-visualizer:
    build:
      context: ../..
      dockerfile: docker/visualizer/Dockerfile
    image: localhost/janusgraph-visualizer:latest
    hostname: janusgraph-visualizer
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - hcd-janusgraph-network
    ports:
      - "3000:3000"    # Web UI
    environment:
      - GREMLIN_URL=ws://janusgraph-server:8182/gremlin
    depends_on:
      janusgraph-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  graphexp:
    build:
      context: ../..
      dockerfile: docker/graphexp/Dockerfile
    image: localhost/graphexp:latest
    hostname: graphexp
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - hcd-janusgraph-network
    ports:
      - "8080:8080"    # Web UI
    depends_on:
      janusgraph-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # ============================================================================
  # CLIENTS
  # ============================================================================

  cqlsh-client:
    build:
      context: ../..
      dockerfile: docker/cqlsh/Dockerfile
    image: localhost/cqlsh-client:latest
    hostname: cqlsh-client
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - hcd-janusgraph-network
    environment:
      - CQLSH_HOST=hcd-server
      - CQLSH_PORT=9042
    depends_on:
      - hcd-server
    stdin_open: true
    tty: true
    restart: "no"
    command: ["tail", "-f", "/dev/null"]  # Keep alive for interactive use

  # ============================================================================
  # SECRETS MANAGEMENT
  # ============================================================================

  vault:
    image: docker.io/hashicorp/vault:1.15.4
    hostname: vault
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - hcd-janusgraph-network
    ports:
      - "8200:8200"    # Vault API
    volumes:
      - vault-data:/vault/file:Z  # Persist vault data in project-scoped volume
      - ./../vault/config.hcl:/vault/config/config.hcl:ro,Z
    environment:
      - VAULT_ADDR=http://0.0.0.0:8200
      - VAULT_API_ADDR=http://0.0.0.0:8200
      - VAULT_LOG_LEVEL=info
      - SKIP_SETCAP=true  # Skip capability setting for rootless
    cap_add:
      - IPC_LOCK
    command: server
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # ============================================================================
  # MONITORING
  # ============================================================================

  prometheus:
    image: docker.io/prom/prometheus:v2.48.1
    hostname: prometheus
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    ports:
      - "9090:9090"    # Prometheus UI
    volumes:
      - ./../monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./../monitoring/alert-rules.yml:/etc/prometheus/alert-rules.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    depends_on:
      - hcd-server
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  alertmanager:
    image: docker.io/prom/alertmanager:v0.26.0
    hostname: alertmanager
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - hcd-janusgraph-network
    ports:
      - "9093:9093"    # AlertManager UI
    volumes:
      - ./../monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    environment:
      - SMTP_PASSWORD=${SMTP_PASSWORD:?SMTP_PASSWORD must be set in .env file}
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL:-https://hooks.slack.com/services/YOUR/WEBHOOK/URL}
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  janusgraph-exporter:
    build:
      context: ../..
      dockerfile: docker/Dockerfile.exporter
    image: localhost/janusgraph-exporter:latest
    hostname: janusgraph-exporter
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - hcd-janusgraph-network
    ports:
      - "8000:8000"    # Metrics endpoint
    environment:
      - GREMLIN_URL=ws://janusgraph-server:8182/gremlin
      - EXPORTER_PORT=8000
      - SCRAPE_INTERVAL=15
    depends_on:
      janusgraph-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  grafana:
    image: docker.io/grafana/grafana:10.2.3
    hostname: grafana
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    ports:
      - "3001:3000"    # Grafana UI (using 3001 to avoid conflict with visualizer)
    volumes:
      - grafana-data:/var/lib/grafana
      - ./../grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./../grafana/datasources:/etc/grafana/provisioning/datasources:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3001
    depends_on:
      - prometheus
      - alertmanager
    restart: unless-stopped

  # ============================================================================
  # ANALYTICS API
  # ============================================================================

  analytics-api:
    build:
      context: ../..
      dockerfile: docker/api/Dockerfile
    image: localhost/analytics-api:latest
    hostname: analytics-api
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    ports:
      - "8001:8001"    # FastAPI
    environment:
      - JANUSGRAPH_HOST=janusgraph-server
      - JANUSGRAPH_PORT=8182
      - API_PORT=8001
      - API_HOST=0.0.0.0
    depends_on:
      janusgraph-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped

  # ============================================================================
  # EVENT CONSUMERS (Pulsar -> JanusGraph/OpenSearch)
  # ============================================================================

  graph-consumer:
    build:
      context: ../..
      dockerfile: docker/consumers/Dockerfile
    image: localhost/graph-consumer:latest
    hostname: graph-consumer
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    environment:
      - PULSAR_URL=pulsar://pulsar:6650
      - JANUSGRAPH_URL=ws://janusgraph-server:8182/gremlin
      - PYTHONUNBUFFERED=1
    command: ["python", "-m", "banking.streaming.graph_consumer"]
    depends_on:
      pulsar:
        condition: service_healthy
      janusgraph-server:
        condition: service_healthy
    restart: unless-stopped

  vector-consumer:
    build:
      context: ../..
      dockerfile: docker/consumers/Dockerfile
    image: localhost/vector-consumer:latest
    hostname: vector-consumer
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    networks:
      - hcd-janusgraph-network
    environment:
      - PULSAR_URL=pulsar://pulsar:6650
      - OPENSEARCH_HOST=opensearch
      - OPENSEARCH_PORT=9200
      - OPENSEARCH_USE_SSL=false
      - PYTHONUNBUFFERED=1
    command: ["python", "-m", "banking.streaming.vector_consumer"]
    depends_on:
      pulsar:
        condition: service_healthy
      opensearch:
        condition: service_healthy
    restart: unless-stopped
